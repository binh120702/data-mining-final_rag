{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"explodinggradients/WikiEval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "api_key = \"your-api-key\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat_handle import ChatHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatHandler = ChatHandler(index_name=\"eval-data-mining-embeddings-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Answer: The PSLV-C56 mission is scheduled to be launched on Sunday, 30 July 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space Centre, Sriharikota, Andhra Pradesh, India.',\n",
       " 'question': 'Question: When is the scheduled launch date and time for the PSLV-C56 mission, and where will it be launched from?',\n",
       " 'context_v1': [\"The PSLV-C56 is the 58th mission of Indian Space Research Organisation's Polar Satellite Launch Vehicle (PSLV) and the 17th flight of the PSLV-CA variant, and will be get launched from Satish Dhawan Space Centre First Launch Pad ( FLP ).\\n\\nLaunch\\nIt is Scheduled to get launched on Sunday, 30 July 2023 at 06:30 IST / 01:00 UTC from Satish Dhawan Space Centre, Sriharikota, Andhra Pradesh, India. This is a dedicated commercial mission through NSIL with DS-SAR as primary satellite and VELOX-AM as a co-passenger satellite With other 5 Satellites, All satellites from this mission belongs to Singapore.\"],\n",
       " 'context_v2': [\"The PSLV-C56 is the 58th mission of Indian Space Research Organisation's Polar Satellite Launch Vehicle (PSLV) and the 17th flight of the PSLV-CA variant, and will be get launched from Satish Dhawan Space Centre First Launch Pad ( FLP ).\\n\\nLaunch\\nIt is Scheduled to get launched on Sunday, 30 July 2023 at 06:30 IST / 01:00 UTC from Satish Dhawan Space Centre, Sriharikota, Andhra Pradesh, India. The Indian Space Research Organisation (ISRO) is the space agency of the Government of India. It was established in 1969 with the aim of developing space technology and conducting space research. ISRO is responsible for the country's space program, which includes satellite launches, space exploration, and the development of space-related technologies.\\n\\nISRO has achieved several significant milestones over the years. It successfully launched its first satellite, Aryabhata, in 1975.\"],\n",
       " 'ungrounded_answer': 'The PSLV-C56 mission is scheduled to be launched on Monday, 30 August 2023. The launch will take place from the Satish Dhawan Space Centre (SDSC) SHAR, located in Sriharikota, India.',\n",
       " 'source': 'PSLV-C56',\n",
       " 'poor_answer': \"The scheduled launch date and time for the PSLV-C56 mission have not been provided.The PSLV-C56 mission is an important space mission for India. It aims to launch a satellite into orbit to study weather patterns.'\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]#['question'][10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(50):\n",
    "#     chatHandler.add_text(ds['train'][i]['context_v1'][0])\n",
    "#     chatHandler.add_text(ds['train'][i]['context_v2'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = chatHandler.chat_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:07<00:00,  8.55s/it]\n"
     ]
    }
   ],
   "source": [
    "chat_responses = []\n",
    "for i in tqdm(range(50)):\n",
    "    chat_responses.append(chat_engine.chat(ds['train'][i]['question'][10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = chat_engine.chat(\"When is the scheduled launch date and time for the PSLV-C56 mission, and where will it be launched from?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.source_nodes[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the gpt-4o-mini judges\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import (\n",
    "    AnswerRelevancyEvaluator,\n",
    "    ContextRelevancyEvaluator,\n",
    ")\n",
    "\n",
    "judges = {}\n",
    "\n",
    "judges[\"answer_relevancy\"] = AnswerRelevancyEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=api_key),\n",
    ")\n",
    "\n",
    "judges[\"context_relevancy\"] = ContextRelevancyEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=api_key),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tasks = []\n",
    "for example, prediction in zip(\n",
    "    ds['train'], chat_responses\n",
    "):\n",
    "    # print('query: ', example['question'][10:])\n",
    "    # print('response: ', prediction.response)\n",
    "    # print('context: ', [x.text for x in prediction.source_nodes])\n",
    "    eval_tasks.append(\n",
    "        judges[\"answer_relevancy\"].aevaluate(\n",
    "            query=example['question'][10:],\n",
    "            response=prediction.response,\n",
    "            sleep_time_in_seconds=1.0,\n",
    "        )\n",
    "    )\n",
    "    eval_tasks.append(\n",
    "        judges[\"context_relevancy\"].aevaluate(\n",
    "            query=example['question'][10:],\n",
    "            contexts=[x.text for x in prediction.source_nodes],\n",
    "            sleep_time_in_seconds=1.0,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.60it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_results1 = await tqdm_asyncio.gather(*eval_tasks[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:11<00:00,  4.26it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_results2 = await tqdm_asyncio.gather(*eval_tasks[50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = eval_results1 + eval_results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = {\n",
    "    \"answer_relevancy\": eval_results[::2],\n",
    "    \"context_relevancy\": eval_results[1::2],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df\n",
    "import pandas as pd\n",
    "\n",
    "deep_dfs = {}\n",
    "mean_dfs = {}\n",
    "for metric in evals.keys():\n",
    "    deep_df, mean_df = get_eval_results_df(\n",
    "        names=[\"baseline\"] * len(evals[metric]),\n",
    "        results_arr=evals[metric],\n",
    "        metric=metric,\n",
    "    )\n",
    "    deep_dfs[metric] = deep_df\n",
    "    mean_dfs[metric] = mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_answer_relevancy_score</th>\n",
       "      <td>0.9800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_context_relevancy_score</th>\n",
       "      <td>0.8925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                           baseline\n",
       "metrics                               \n",
       "mean_answer_relevancy_score     0.9800\n",
       "mean_context_relevancy_score    0.8925"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_scores_df = pd.concat(\n",
    "    [mdf.reset_index() for _, mdf in mean_dfs.items()],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Response,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4\n",
    "gpt4 = OpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "\n",
    "evaluator_gpt4 = FaithfulnessEvaluator(llm=gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import EvaluationResult\n",
    "\n",
    "\n",
    "# define jupyter display function\n",
    "def display_eval_df(response: Response, eval_result: EvaluationResult) -> None:\n",
    "    if response.source_nodes == []:\n",
    "        print(\"no response!\")\n",
    "        return\n",
    "    eval_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Response\": str(response),\n",
    "            \"Source\": response.source_nodes[0].node.text[:1000] + \"...\",\n",
    "            \"Evaluation Result\": \"Pass\" if eval_result.passing else \"Fail\",\n",
    "            \"Reasoning\": eval_result.feedback,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    eval_df = eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"600px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Response\", \"Source\"]\n",
    "    )\n",
    "    display(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = [s['question'][10:] for s in ds['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When is the scheduled launch date and time for the PSLV-C56 mission, and where will it be launched from?',\n",
       " 'What is the objective of the Uzbekistan-Afghanistan-Pakistan Railway Project and how is it expected to enhance trade and logistics efficiency?',\n",
       " 'When was PharmaCann founded and what is its headquarters location?',\n",
       " 'Who directed the film Oppenheimer and who stars as J. Robert Oppenheimer in the film?',\n",
       " 'What is theranostics and how does it combine diagnostic and therapeutic approaches in precision medicine?']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "def evaluate_query_engine(query_engine, questions):\n",
    "    c = [query_engine.aquery(q) for q in questions]\n",
    "    results = asyncio.run(asyncio.gather(*c))\n",
    "    print(\"finished query\")\n",
    "\n",
    "    total_correct = 0\n",
    "    for r in results:\n",
    "        # evaluate with gpt 4\n",
    "        eval_result = (\n",
    "            1 if evaluator_gpt4.evaluate_response(response=r).passing else 0\n",
    "        )\n",
    "        total_correct += eval_result\n",
    "\n",
    "    return total_correct, len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = chatHandler.index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished query\n",
      "score: 50/50\n"
     ]
    }
   ],
   "source": [
    "correct, total = evaluate_query_engine(query_engine, eval_questions[:50])\n",
    "\n",
    "print(f\"score: {correct}/{total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
